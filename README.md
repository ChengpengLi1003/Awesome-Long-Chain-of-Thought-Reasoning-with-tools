# Awesome-Long-Chain-of-Thought-Reasoning-with-tools
A curated list of cutting-edge research papers and resources on Long Chain-of-Thought (CoT) Reasoning with Tools.

[English] | [‰∏≠Êñá](READ_CN.md)

-----


# Awesome Long-Chain-of-Thought Reasoning with Tools ü§ñÔ∏è‚õìÔ∏èüõ†Ô∏è

[](https://www.google.com/search?q=%5Bhttps://awesome.re%5D\(https://awesome.re\))

A curated list of cutting-edge research papers and resources on **Long Chain-of-Thought (CoT) Reasoning with Tools**.

In recent years, Large Reasoning Models (LRMs) have made significant strides in Long-Chain-of-Thought (Long-CoT) reasoning. However, relying solely on a model's internal knowledge often leads to hallucinations and inefficiencies. To address these issues, both academia and industry have started exploring methods to integrate external tools (such as code interpreters, search engines, APIs, etc.) into the reasoning process. This repository aims to collect and organize outstanding work in this field, focusing on research that enhances model capabilities in computation, self-consistency checking, information retrieval, and complex task-solving through the use of tools.

-----

## Table of Contents

  - [‚ú® Hint-based & Fine-tuning Methods](https://www.google.com/search?q=%23-hint-based--fine-tuning-methods)
  - [üß† Reinforcement Learning Methods](https://www.google.com/search?q=%23-reinforcement-learning-methods)
  - [üåê Agentic Search & Exploration Methods](https://www.google.com/search?q=%23-agentic-search--exploration-methods)

-----

## ‚ú® Hint-based & Fine-tuning Methods

These methods use cleverly designed hints or efficient fine-tuning frameworks to elicit and train a model's ability to use external tools, often without requiring large amounts of manually annotated data.

### START: Self-taught Reasoner with Tools

  - üìÑ **Paper:** [https://arxiv.org/abs/2503.04625](https://arxiv.org/abs/2503.04625)
  - üíª **Code:** [https://github.com/ChengpengLi1003/CoRT](https://github.com/ChengpengLi1003/CoRT)
  - **Abstract:** START is a novel tool-integrated long CoT reasoning LLM. It performs complex computations, self-checks, and debugs through code execution. Its core innovation is a self-learning framework with two key techniques: 1) **Hint-infer**: Inserting artificially designed hints (e.g., "Wait, maybe using Python here is a good idea.") during inference to stimulate the model's tool-use ability. 2) **Hint Rejection Sampling Fine-Tuning (Hint-RFT)**: Scoring, filtering, and modifying reasoning trajectories with tool invocation generated by Hint-infer, then fine-tuning the model on this data. After fine-tuning the QwQ-32B model, START achieves performance comparable to state-of-the-art models on several difficult science, math, and coding benchmarks.

### An empirical study on eliciting and improving r1-like reasoning models

  - üìÑ **Paper:** [https://arxiv.org/abs/2503.04548](https://arxiv.org/abs/2503.04548)
  - üíª **Code:** [https://github.com/RUCAIBox/Slow\_Thinking\_with\_LLMs](https://github.com/RUCAIBox/Slow_Thinking_with_LLMs)
  - **Abstract:** This technical report systematically experiments with and documents various factors influencing Reinforcement Learning (RL) training for reasoning models. The study shows that RL training consistently improves the Qwen2.5-32B base models. Furthermore, it explores the use of **tool manipulation**, which significantly boosts the reasoning performance of large reasoning models, achieving a remarkable 86.67% accuracy with greedy search on AIME 2024, underscoring its effectiveness in enhancing model capabilities.

### Agentic-R1: Distilled Dual-Strategy Reasoning

  - üìÑ **Paper:** [https://arxiv.org/abs/2507.05707](https://arxiv.org/abs/2507.05707)
  - üíª **Code:** [https://github.com/StigLidu/DualDistill](https://www.google.com/search?q=https://github.com/StigLidu/DualDistill)
  - **Abstract:** To address the respective shortcomings of pure text-based reasoning and tool-augmented agents, this work introduces the **DualDistill** framework. It distills complementary reasoning strategies (textual reasoning and tool use) from multiple teacher models into a single student model. The resulting **Agentic-R1** can dynamically select the optimal strategy for each query: invoking tools for arithmetic and algorithmic problems, and using text-based reasoning for abstract ones. This multi-strategy distillation approach improves accuracy across a range of tasks, achieving robust and efficient reasoning.

-----

## üß† Reinforcement Learning Methods

These methods leverage reinforcement learning by using task outcomes (e.g., correctness, efficiency) as reward signals, allowing the model to autonomously learn when and how to call tools to optimize its problem-solving strategy.

### CoRT: Code-integrated Reasoning within Thinking

  - üìÑ **Paper:** [https://arxiv.org/abs/2506.09820](https://arxiv.org/abs/2506.09820)
  - üíª **Code:** [https://github.com/RUCAIBox/Slow\_Thinking\_with\_LLMs](https://github.com/RUCAIBox/Slow_Thinking_with_LLMs)
  - **Abstract:** CoRT is a post-training framework designed to teach LRMs to effectively and efficiently leverage a Code Interpreter (CI). It addresses data scarcity by synthesizing code-integrated reasoning data through **Hint-Engineering**. Based on 30 high-quality manually created samples, models ranging from 1.5B to 32B parameters were post-trained using supervised fine-tuning, rejection fine-tuning, and **reinforcement learning**. Experiments show that this method achieves significant absolute performance improvements (4% for 32B model, 8% for 1.5B model) on challenging math reasoning datasets while substantially reducing the number of tokens required.

### Retool: Reinforcement learning for strategic tool use in llms

  - üìÑ **Paper:** [https://arxiv.org/abs/2504.11536](https://arxiv.org/abs/2504.11536)
  - **Abstract:** The **ReTool** framework enhances long-form reasoning with RL. Its key features include: 1) Dynamically interleaving real-time code execution within natural language reasoning; 2) An automated RL paradigm that teaches the model when and how to invoke tools based on outcome feedback. ReTool starts with fine-tuning on synthetic data, followed by RL training using task outcomes as rewards to iteratively refine the model's tool-use strategy. On the AIME benchmark, Retool-32B achieved 67% accuracy with fewer training steps (400) compared to a text-based RL baseline (40% accuracy, 1080 steps), and exhibited emergent behaviors like code self-correction.

### Kimi-Researcher: End-to-End Agentic RL for Autonomous Reasoning

  - üìÑ **Homepage:** [https://moonshotai.github.io/Kimi-Researcher/](https://moonshotai.github.io/Kimi-Researcher/)
  - **Abstract:** **Kimi-Researcher** is an autonomous agent excelling at multi-turn search and reasoning, trained entirely through end-to-end agentic reinforcement learning (RL). It performs an average of 23 reasoning steps and explores over 200 URLs per task. It achieved a state-of-the-art Pass@1 score of 26.9% on the highly challenging **Humanity's Last Exam (HLE)** benchmark, providing compelling evidence that end-to-end agentic RL can significantly advance agent intelligence.

-----

## üåê Agentic Search & Exploration Methods

These methods build agents that can actively interact with external environments (like the web or documents) to compensate for their own knowledge gaps through dynamic search and information extraction, enabling them to complete complex, knowledge-intensive reasoning tasks.

### Agentic Search-Enhanced Large Reasoning Models

  - üìÑ **Paper:** [https://arxiv.org/abs/2501.05366](https://arxiv.org/abs/2501.05366)
  - üíª **Code:** [https://github.com/sunnynexus/Search-o1](https://github.com/sunnynexus/Search-o1)
  - **Abstract:** The **Search-o1** framework enhances LRMs with an **agentic retrieval-augmented generation (RAG)** mechanism. It integrates an agentic search workflow into the reasoning process, allowing the model to dynamically retrieve external knowledge when encountering uncertain points. Additionally, it features a separate **Reason-in-Documents** module to deeply analyze retrieved information before injecting it into the reasoning chain, minimizing noise and maintaining coherent flow.

### WebThinker: Empowering Large Reasoning Models with Deep Research Capability

  - üìÑ **Paper:** [https://arxiv.org/abs/2504.21776](https://arxiv.org/abs/2504.21776)
  - üíª **Code:** [https://github.com/RUC-NLPIR/WebThinker](https://github.com/RUC-NLPIR/WebThinker)
  - **Abstract:** **WebThinker** is a deep research agent that empowers LRMs to autonomously search the web, navigate pages, and draft research reports during reasoning. The framework integrates a **Deep Web Explorer** module and employs an **Autonomous Think-Search-and-Draft** strategy, enabling the model to seamlessly interleave reasoning, information gathering, and report writing. It also uses an RL-based online DPO strategy to enhance research tool utilization.

### SciMaster: Towards General-Purpose Scientific AI Agents

  - üìÑ **Paper:** [https://arxiv.org/abs/2507.05241](https://arxiv.org/abs/2507.05241)
  - üíª **Code:** [https://github.com/sjtu-sai-agents/X-Master](https://github.com/sjtu-sai-agents/X-Master)
  - **Abstract:** **X-Master** is a tool-augmented reasoning agent designed to emulate human researchers by conceptualizing code as an interaction language, flexibly leveraging Python libraries and custom tools. Through a scattered-and-stacked agentic workflow called **X-Masters**, the system systematically enhances the breadth and depth of reasoning. Its open-source solution set a new state-of-the-art record on the **HLE** benchmark with a score of 32.1%, becoming the first to exceed the 30% threshold and surpassing similar research from OpenAI and Google.

-----

\<br\>
