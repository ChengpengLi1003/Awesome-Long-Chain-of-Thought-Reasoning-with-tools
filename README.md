# Awesome-Long-Chain-of-Thought-Reasoning-with-tools
A curated list of cutting-edge research papers and resources on Long Chain-of-Thought (CoT) Reasoning with Tools.

[English](https://www.google.com/search?q=%23english) | [ä¸­æ–‡](https://www.google.com/search?q=%23%E4%B8%AD%E6%96%87)

-----

\<a name="english"\>\</a\>

# Awesome Long-Chain-of-Thought Reasoning with Tools ğŸ¤–ï¸â›“ï¸ğŸ› ï¸

[](https://www.google.com/search?q=%5Bhttps://awesome.re%5D\(https://awesome.re\))

A curated list of cutting-edge research papers and resources on **Long Chain-of-Thought (CoT) Reasoning with Tools**.

In recent years, Large Reasoning Models (LRMs) have made significant strides in Long-Chain-of-Thought (Long-CoT) reasoning. However, relying solely on a model's internal knowledge often leads to hallucinations and inefficiencies. To address these issues, both academia and industry have started exploring methods to integrate external tools (such as code interpreters, search engines, APIs, etc.) into the reasoning process. This repository aims to collect and organize outstanding work in this field, focusing on research that enhances model capabilities in computation, self-consistency checking, information retrieval, and complex task-solving through the use of tools.

-----

## Table of Contents

  - [âœ¨ Hint-based & Fine-tuning Methods](https://www.google.com/search?q=%23-hint-based--fine-tuning-methods)
  - [ğŸ§  Reinforcement Learning Methods](https://www.google.com/search?q=%23-reinforcement-learning-methods)
  - [ğŸŒ Agentic Search & Exploration Methods](https://www.google.com/search?q=%23-agentic-search--exploration-methods)

-----

## âœ¨ Hint-based & Fine-tuning Methods

These methods use cleverly designed hints or efficient fine-tuning frameworks to elicit and train a model's ability to use external tools, often without requiring large amounts of manually annotated data.

### START: Self-taught Reasoner with Tools

  - ğŸ“„ **Paper:** [https://arxiv.org/abs/2503.04625](https://arxiv.org/abs/2503.04625)
  - ğŸ’» **Code:** [https://github.com/ChengpengLi1003/CoRT](https://github.com/ChengpengLi1003/CoRT)
  - **Abstract:** START is a novel tool-integrated long CoT reasoning LLM. It performs complex computations, self-checks, and debugs through code execution. Its core innovation is a self-learning framework with two key techniques: 1) **Hint-infer**: Inserting artificially designed hints (e.g., "Wait, maybe using Python here is a good idea.") during inference to stimulate the model's tool-use ability. 2) **Hint Rejection Sampling Fine-Tuning (Hint-RFT)**: Scoring, filtering, and modifying reasoning trajectories with tool invocation generated by Hint-infer, then fine-tuning the model on this data. After fine-tuning the QwQ-32B model, START achieves performance comparable to state-of-the-art models on several difficult science, math, and coding benchmarks.

### An empirical study on eliciting and improving r1-like reasoning models

  - ğŸ“„ **Paper:** [https://arxiv.org/abs/2503.04548](https://arxiv.org/abs/2503.04548)
  - ğŸ’» **Code:** [https://github.com/RUCAIBox/Slow\_Thinking\_with\_LLMs](https://github.com/RUCAIBox/Slow_Thinking_with_LLMs)
  - **Abstract:** This technical report systematically experiments with and documents various factors influencing Reinforcement Learning (RL) training for reasoning models. The study shows that RL training consistently improves the Qwen2.5-32B base models. Furthermore, it explores the use of **tool manipulation**, which significantly boosts the reasoning performance of large reasoning models, achieving a remarkable 86.67% accuracy with greedy search on AIME 2024, underscoring its effectiveness in enhancing model capabilities.

### Agentic-R1: Distilled Dual-Strategy Reasoning

  - ğŸ“„ **Paper:** [https://arxiv.org/abs/2507.05707](https://arxiv.org/abs/2507.05707)
  - ğŸ’» **Code:** [https://github.com/StigLidu/DualDistill](https://www.google.com/search?q=https://github.com/StigLidu/DualDistill)
  - **Abstract:** To address the respective shortcomings of pure text-based reasoning and tool-augmented agents, this work introduces the **DualDistill** framework. It distills complementary reasoning strategies (textual reasoning and tool use) from multiple teacher models into a single student model. The resulting **Agentic-R1** can dynamically select the optimal strategy for each query: invoking tools for arithmetic and algorithmic problems, and using text-based reasoning for abstract ones. This multi-strategy distillation approach improves accuracy across a range of tasks, achieving robust and efficient reasoning.

-----

## ğŸ§  Reinforcement Learning Methods

These methods leverage reinforcement learning by using task outcomes (e.g., correctness, efficiency) as reward signals, allowing the model to autonomously learn when and how to call tools to optimize its problem-solving strategy.

### CoRT: Code-integrated Reasoning within Thinking

  - ğŸ“„ **Paper:** [https://arxiv.org/abs/2506.09820](https://arxiv.org/abs/2506.09820)
  - ğŸ’» **Code:** [https://github.com/RUCAIBox/Slow\_Thinking\_with\_LLMs](https://github.com/RUCAIBox/Slow_Thinking_with_LLMs)
  - **Abstract:** CoRT is a post-training framework designed to teach LRMs to effectively and efficiently leverage a Code Interpreter (CI). It addresses data scarcity by synthesizing code-integrated reasoning data through **Hint-Engineering**. Based on 30 high-quality manually created samples, models ranging from 1.5B to 32B parameters were post-trained using supervised fine-tuning, rejection fine-tuning, and **reinforcement learning**. Experiments show that this method achieves significant absolute performance improvements (4% for 32B model, 8% for 1.5B model) on challenging math reasoning datasets while substantially reducing the number of tokens required.

### Retool: Reinforcement learning for strategic tool use in llms

  - ğŸ“„ **Paper:** [https://arxiv.org/abs/2504.11536](https://arxiv.org/abs/2504.11536)
  - **Abstract:** The **ReTool** framework enhances long-form reasoning with RL. Its key features include: 1) Dynamically interleaving real-time code execution within natural language reasoning; 2) An automated RL paradigm that teaches the model when and how to invoke tools based on outcome feedback. ReTool starts with fine-tuning on synthetic data, followed by RL training using task outcomes as rewards to iteratively refine the model's tool-use strategy. On the AIME benchmark, Retool-32B achieved 67% accuracy with fewer training steps (400) compared to a text-based RL baseline (40% accuracy, 1080 steps), and exhibited emergent behaviors like code self-correction.

### Kimi-Researcher: End-to-End Agentic RL for Autonomous Reasoning

  - ğŸ“„ **Homepage:** [https://moonshotai.github.io/Kimi-Researcher/](https://moonshotai.github.io/Kimi-Researcher/)
  - **Abstract:** **Kimi-Researcher** is an autonomous agent excelling at multi-turn search and reasoning, trained entirely through end-to-end agentic reinforcement learning (RL). It performs an average of 23 reasoning steps and explores over 200 URLs per task. It achieved a state-of-the-art Pass@1 score of 26.9% on the highly challenging **Humanity's Last Exam (HLE)** benchmark, providing compelling evidence that end-to-end agentic RL can significantly advance agent intelligence.

-----

## ğŸŒ Agentic Search & Exploration Methods

These methods build agents that can actively interact with external environments (like the web or documents) to compensate for their own knowledge gaps through dynamic search and information extraction, enabling them to complete complex, knowledge-intensive reasoning tasks.

### Agentic Search-Enhanced Large Reasoning Models

  - ğŸ“„ **Paper:** [https://arxiv.org/abs/2501.05366](https://arxiv.org/abs/2501.05366)
  - ğŸ’» **Code:** [https://github.com/sunnynexus/Search-o1](https://github.com/sunnynexus/Search-o1)
  - **Abstract:** The **Search-o1** framework enhances LRMs with an **agentic retrieval-augmented generation (RAG)** mechanism. It integrates an agentic search workflow into the reasoning process, allowing the model to dynamically retrieve external knowledge when encountering uncertain points. Additionally, it features a separate **Reason-in-Documents** module to deeply analyze retrieved information before injecting it into the reasoning chain, minimizing noise and maintaining coherent flow.

### WebThinker: Empowering Large Reasoning Models with Deep Research Capability

  - ğŸ“„ **Paper:** [https://arxiv.org/abs/2504.21776](https://arxiv.org/abs/2504.21776)
  - ğŸ’» **Code:** [https://github.com/RUC-NLPIR/WebThinker](https://github.com/RUC-NLPIR/WebThinker)
  - **Abstract:** **WebThinker** is a deep research agent that empowers LRMs to autonomously search the web, navigate pages, and draft research reports during reasoning. The framework integrates a **Deep Web Explorer** module and employs an **Autonomous Think-Search-and-Draft** strategy, enabling the model to seamlessly interleave reasoning, information gathering, and report writing. It also uses an RL-based online DPO strategy to enhance research tool utilization.

### SciMaster: Towards General-Purpose Scientific AI Agents

  - ğŸ“„ **Paper:** [https://arxiv.org/abs/2507.05241](https://arxiv.org/abs/2507.05241)
  - ğŸ’» **Code:** [https://github.com/sjtu-sai-agents/X-Master](https://github.com/sjtu-sai-agents/X-Master)
  - **Abstract:** **X-Master** is a tool-augmented reasoning agent designed to emulate human researchers by conceptualizing code as an interaction language, flexibly leveraging Python libraries and custom tools. Through a scattered-and-stacked agentic workflow called **X-Masters**, the system systematically enhances the breadth and depth of reasoning. Its open-source solution set a new state-of-the-art record on the **HLE** benchmark with a score of 32.1%, becoming the first to exceed the 30% threshold and surpassing similar research from OpenAI and Google.

-----

\<br\>
\<a name="chinese"\>\</a\>

# Awesome Long-Chain-of-Thought Reasoning with Tools ğŸ¤–ï¸â›“ï¸ğŸ› ï¸ (ä¸­æ–‡)

[](https://www.google.com/search?q=%5Bhttps://awesome.re%5D\(https://awesome.re\))

ä¸€ä¸ªç²¾é€‰çš„ã€å…³äº**å¸¦å·¥å…·çš„é•¿é“¾æ€ç»´æ¨ç† (Long Chain-of-Thought Reasoning with Tools)** çš„å‰æ²¿ç ”ç©¶è®ºæ–‡å’Œèµ„æºçš„åˆ—è¡¨ã€‚

è¿‘å¹´æ¥ï¼Œå¤§å‹æ¨ç†æ¨¡å‹ (Large Reasoning Models, LRMs) åœ¨é•¿é“¾æ€ç»´ (Long-CoT) æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚ç„¶è€Œï¼Œå•çº¯ä¾èµ–æ¨¡å‹å†…éƒ¨çŸ¥è¯†çš„æ¨ç†è¿‡ç¨‹å¸¸å¸¸å¯¼è‡´å¹»è§‰å’Œæ•ˆç‡ä½ä¸‹ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œå­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œå¼€å§‹æ¢ç´¢å°†å¤–éƒ¨å·¥å…·ï¼ˆå¦‚ä»£ç è§£é‡Šå™¨ã€æœç´¢å¼•æ“ã€APIç­‰ï¼‰é›†æˆåˆ°æ¨ç†è¿‡ç¨‹ä¸­çš„æ–¹æ³•ã€‚æœ¬ä»“åº“æ—¨åœ¨æ”¶é›†å’Œæ•´ç†è¯¥é¢†åŸŸçš„æ°å‡ºå·¥ä½œï¼Œé‡ç‚¹å…³æ³¨é‚£äº›é€šè¿‡å·¥å…·ä½¿ç”¨æ¥å¢å¼ºæ¨¡å‹è®¡ç®—ã€è‡ªæ´½æ€§æ£€æŸ¥ã€ä¿¡æ¯æ£€ç´¢å’Œå¤æ‚ä»»åŠ¡è§£å†³èƒ½åŠ›çš„ç ”ç©¶ã€‚

-----

## ç›®å½•

  - [âœ¨ åŸºäºæç¤ºå·¥ç¨‹ä¸å¾®è°ƒçš„æ–¹æ³•](https://www.google.com/search?q=%23-%E5%9F%BA%E4%BA%8E%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B%E4%B8%8E%E5%BE%AE%E8%B0%83%E7%9A%84%E6%96%B9%E6%B3%95-1)
  - [ğŸ§  åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•](https://www.google.com/search?q=%23-%E5%9F%BA%E4%BA%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%B9%E6%B3%95-1)
  - [ğŸŒ åŸºäºæ™ºèƒ½ä½“æœç´¢ä¸æ¢ç´¢çš„æ–¹æ³•](https://www.google.com/search?q=%23-%E5%9F%BA%E4%BA%8E%E6%99%BA%E8%83%BD%E4%BD%93%E6%90%9C%E7%B4%A2%E4%B8%8E%E6%8E%A2%E7%B4%A2%E7%9A%84%E6%96%B9%E6%B3%95-1)

-----

## âœ¨ åŸºäºæç¤ºå·¥ç¨‹ä¸å¾®è°ƒçš„æ–¹æ³•

è¿™ç±»æ–¹æ³•é€šè¿‡å·§å¦™è®¾è®¡çš„æç¤ºï¼ˆHintsï¼‰æˆ–è€…é«˜æ•ˆçš„å¾®è°ƒæ¡†æ¶ï¼Œåœ¨æ— éœ€å¤§é‡äººå·¥æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæ¿€å‘å’Œè®­ç»ƒæ¨¡å‹ä½¿ç”¨å¤–éƒ¨å·¥å…·çš„èƒ½åŠ›ã€‚

### START: Self-taught Reasoner with Tools

  - ğŸ“„ **è®ºæ–‡:** [https://arxiv.org/abs/2503.04625](https://arxiv.org/abs/2503.04625)
  - ğŸ’» **ä»£ç :** [https://github.com/ChengpengLi1003/CoRT](https://github.com/ChengpengLi1003/CoRT)
  - **æ‘˜è¦:** START æ˜¯ä¸€ç§æ–°é¢–çš„ã€é›†æˆäº†å·¥å…·çš„é•¿é“¾æ€ç»´æ¨ç†æ¨¡å‹ã€‚å®ƒé€šè¿‡ä»£ç æ‰§è¡Œæ¥è¿›è¡Œå¤æ‚è®¡ç®—ã€è‡ªæˆ‘æ£€æŸ¥å’Œè°ƒè¯•ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºä¸€ä¸ªè‡ªå­¦ä¹ æ¡†æ¶ï¼ŒåŒ…å«ä¸¤ä¸ªå…³é”®æŠ€æœ¯ï¼š1) **Hint-infer**ï¼šåœ¨æ¨ç†æ—¶æ’å…¥äººå·¥è®¾è®¡çš„æç¤ºï¼ˆå¦‚â€œç­‰ç­‰ï¼Œè¿™é‡Œä½¿ç”¨Pythonå¯èƒ½æ˜¯ä¸ªå¥½ä¸»æ„â€ï¼‰æ¥æ¿€å‘æ¨¡å‹ä½¿ç”¨å·¥å…·çš„èƒ½åŠ›ã€‚2) **Hint Rejection Sampling Fine-Tuning (Hint-RFT)**ï¼šç»“åˆ Hint-infer ç”Ÿæˆçš„å¸¦å·¥å…·è°ƒç”¨çš„æ¨ç†è½¨è¿¹ï¼Œè¿›è¡Œæ‰“åˆ†ã€è¿‡æ»¤å’Œä¿®æ”¹ï¼Œç„¶åå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚é€šè¿‡è¯¥æ¡†æ¶ï¼ŒQwQ-32B æ¨¡å‹ç»è¿‡å¾®è°ƒåï¼Œåœ¨å¤šä¸ªé«˜éš¾åº¦ç§‘å­¦ã€æ•°å­¦å’Œç¼–ç¨‹åŸºå‡†ä¸Šå–å¾—äº†ä¸æœ€å…ˆè¿›æ¨¡å‹ç›¸åª²ç¾çš„æ€§èƒ½ã€‚

### An empirical study on eliciting and improving r1-like reasoning models

  - ğŸ“„ **è®ºæ–‡:** [https://arxiv.org/abs/2503.04548](https://arxiv.org/abs/2503.04548)
  - ğŸ’» **ä»£ç :** [https://github.com/RUCAIBox/Slow\_Thinking\_with\_LLMs](https://github.com/RUCAIBox/Slow_Thinking_with_LLMs)
  - **æ‘˜è¦:** è¿™ä»½æŠ€æœ¯æŠ¥å‘Šç³»ç»Ÿæ€§åœ°å®éªŒå’Œè®°å½•äº†å½±å“å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒçš„å„ç§å› ç´ ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒRLè®­ç»ƒèƒ½å¤ŸæŒç»­æ”¹è¿›Qwen2.5-32BåŸºç¡€æ¨¡å‹ï¼Œæå‡å…¶å“åº”é•¿åº¦å’Œæµ‹è¯•å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼ŒæŠ¥å‘Šè¿˜æ¢ç´¢äº†**å·¥å…·æ“ä½œ (tool manipulation)** çš„ä½¿ç”¨ï¼Œå‘ç°å®ƒèƒ½æ˜¾è‘—æå‡å¤§å‹æ¨ç†æ¨¡å‹çš„æ¨ç†æ€§èƒ½ï¼Œåœ¨AIME 2024ä¸Šé€šè¿‡è´ªå¿ƒæœç´¢è¾¾åˆ°äº†86.67%çš„æƒŠäººå‡†ç¡®ç‡ï¼Œè¯æ˜äº†å…¶åœ¨å¢å¼ºæ¨¡å‹èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚

### Agentic-R1: Distilled Dual-Strategy Reasoning

  - ğŸ“„ **è®ºæ–‡:** [https://arxiv.org/abs/2507.05707](https://arxiv.org/abs/2507.05707)
  - ğŸ’» **ä»£ç :** [https://github.com/StigLidu/DualDistill](https://www.google.com/search?q=https://github.com/StigLidu/DualDistill)
  - **æ‘˜è¦:** ä¸ºè§£å†³çº¯æ–‡æœ¬æ¨ç†å’Œå·¥å…·å¢å¼ºæ™ºèƒ½ä½“å„è‡ªçš„çŸ­æ¿ï¼Œè¯¥å·¥ä½œæå‡ºäº† **DualDistill** æ¡†æ¶ï¼Œå°†å¤šä¸ªæ•™å¸ˆæ¨¡å‹çš„äº’è¡¥æ¨ç†ç­–ç•¥ï¼ˆæ–‡æœ¬æ¨ç†ä¸å·¥å…·ä½¿ç”¨ï¼‰è’¸é¦åˆ°ä¸€ä¸ªç»Ÿä¸€çš„å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚ç”±æ­¤è®­ç»ƒå‡ºçš„ **Agentic-R1** èƒ½å¤Ÿä¸ºæ¯ä¸ªæŸ¥è¯¢åŠ¨æ€é€‰æ‹©æœ€ä¼˜ç­–ç•¥ï¼šå¯¹ç®—æœ¯å’Œç®—æ³•é—®é¢˜è°ƒç”¨å·¥å…·ï¼Œå¯¹æŠ½è±¡é—®é¢˜ä½¿ç”¨æ–‡æœ¬æ¨ç†ã€‚è¿™ç§å¤šç­–ç•¥è’¸é¦æ–¹æ³•åœ¨è®¡ç®—å¯†é›†å‹å’Œæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸­å‡æé«˜äº†å‡†ç¡®æ€§ï¼Œå®ç°äº†å¼ºå¤§è€Œé«˜æ•ˆçš„æ¨ç†ã€‚

-----

## ğŸ§  åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ–¹æ³•

è¿™ç±»æ–¹æ³•åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡å°†ä»»åŠ¡ç»“æœï¼ˆå¦‚æ­£ç¡®æ€§ã€æ•ˆç‡ï¼‰ä½œä¸ºå¥–åŠ±ä¿¡å·ï¼Œè®©æ¨¡å‹è‡ªä¸»å­¦ä¹ ä½•æ—¶ä»¥åŠå¦‚ä½•è°ƒç”¨å·¥å…·æ¥ä¼˜åŒ–å…¶é—®é¢˜è§£å†³ç­–ç•¥ã€‚

### CoRT: Code-integrated Reasoning within Thinking

  - ğŸ“„ **è®ºæ–‡:** [https://arxiv.org/abs/2506.09820](https://arxiv.org/abs/2506.09820)
  - ğŸ’» **ä»£ç :** [https://github.com/RUCAIBox/Slow\_Thinking\_with\_LLMs](https://github.com/RUCAIBox/Slow_Thinking_with_LLMs)
  - **æ‘˜è¦:** CoRT æ˜¯ä¸€ä¸ªåè®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨æ•™ä¼šå¤§å‹æ¨ç†æ¨¡å‹é«˜æ•ˆåœ°åˆ©ç”¨ä»£ç è§£é‡Šå™¨ï¼ˆCIï¼‰ã€‚è¯¥å·¥ä½œé€šè¿‡**æç¤ºå·¥ç¨‹ (Hint-Engineering)** æ¥åˆæˆä»£ç é›†æˆçš„æ¨ç†æ•°æ®ï¼Œå¹¶åœ¨30ä¸ªé«˜è´¨é‡æ ·æœ¬åŸºç¡€ä¸Šï¼Œå¯¹æ¨¡å‹è¿›è¡Œäº†ç›‘ç£å¾®è°ƒã€æ‹’ç»é‡‡æ ·å¾®è°ƒå’Œ**å¼ºåŒ–å­¦ä¹ **ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæŒ‘æˆ˜æ€§æ•°å­¦æ¨ç†æ•°æ®é›†ä¸Šå–å¾—äº†æ˜¾è‘—çš„ç»å¯¹æ€§èƒ½æå‡ï¼ˆ32Bæ¨¡å‹æå‡4%ï¼Œ1.5Bæ¨¡å‹æå‡8%ï¼‰ï¼Œå¹¶ä¸”å¤§å¹…å‡å°‘äº†ç”Ÿæˆæ‰€éœ€çš„Tokenæ•°é‡ã€‚

### Retool: Reinforcement learning for strategic tool use in llms

  - ğŸ“„ **è®ºæ–‡:** [https://arxiv.org/abs/2504.11536](https://arxiv.org/abs/2504.11536)
  - **æ‘˜è¦:** **ReTool** æ¡†æ¶é€šè¿‡å¼ºåŒ–å­¦ä¹ æ¥å¢å¼ºæ¨¡å‹çš„é•¿ç¨‹æ¨ç†èƒ½åŠ›ã€‚å…¶ç‰¹ç‚¹åŒ…æ‹¬ï¼š1) åœ¨è‡ªç„¶è¯­è¨€æ¨ç†è¿‡ç¨‹ä¸­åŠ¨æ€äº¤ç»‡å®æ—¶ä»£ç æ‰§è¡Œï¼›2) ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„RLèŒƒå¼ï¼Œå…è®¸æ¨¡å‹æ ¹æ®ç»“æœåé¦ˆå­¦ä¹ ä½•æ—¶ä»¥åŠå¦‚ä½•è°ƒç”¨å·¥å…·ã€‚ReTool ä»åˆæˆçš„å†·å¯åŠ¨æ•°æ®å¼€å§‹è¿›è¡Œå¾®è°ƒï¼Œéšåé€šè¿‡RLè®­ç»ƒï¼Œåˆ©ç”¨ä»»åŠ¡ç»“æœä½œä¸ºå¥–åŠ±æ¥è¿­ä»£ä¼˜åŒ–æ¨¡å‹çš„å·¥å…·ä½¿ç”¨ç­–ç•¥ã€‚åœ¨AIMEåŸºå‡†æµ‹è¯•ä¸­ï¼ŒReTool-32Bä»¥æ›´å°‘çš„è®­ç»ƒæ­¥æ•°ï¼ˆ400æ­¥ï¼‰è¾¾åˆ°äº†67%çš„å‡†ç¡®ç‡ï¼Œè¿œè¶…åŸºäºæ–‡æœ¬çš„RLåŸºçº¿ï¼ˆ40%å‡†ç¡®ç‡ï¼Œ1080æ­¥ï¼‰ï¼Œå¹¶æ¶Œç°å‡ºä»£ç è‡ªæˆ‘çº æ­£ç­‰é«˜çº§è¡Œä¸ºã€‚

### Kimi-Researcher: End-to-End Agentic RL for Autonomous Reasoning

  - ğŸ“„ **ä¸»é¡µ:** [https://moonshotai.github.io/Kimi-Researcher/](https://moonshotai.github.io/Kimi-Researcher/)
  - **æ‘˜è¦:** **Kimi-Researcher** æ˜¯ä¸€ä¸ªæ“…é•¿å¤šè½®æœç´¢å’Œæ¨ç†çš„è‡ªä¸»æ™ºèƒ½ä½“ï¼Œå®Œå…¨é€šè¿‡ç«¯åˆ°ç«¯çš„æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è¿›è¡Œè®­ç»ƒã€‚å®ƒåœ¨æ¯é¡¹ä»»åŠ¡ä¸­å¹³å‡æ‰§è¡Œ23ä¸ªæ¨ç†æ­¥éª¤ï¼Œæ¢ç´¢è¶…è¿‡200ä¸ªURLã€‚åœ¨æå…·æŒ‘æˆ˜æ€§çš„ **Humanity's Last Exam (HLE)** åŸºå‡†æµ‹è¯•ä¸­ï¼Œå®ƒå–å¾—äº†26.9%çš„ Pass@1 åˆ†æ•°ï¼Œè¾¾åˆ°äº†ä¸–ç•Œé¢†å…ˆæ°´å¹³ã€‚è¿™ä¸€ç»“æœæœ‰åŠ›åœ°è¯æ˜äº†ç«¯åˆ°ç«¯çš„æ™ºèƒ½ä½“RLèƒ½å¤Ÿæ˜¾è‘—æå‡æ™ºèƒ½ä½“çš„æ™ºèƒ½æ°´å¹³ã€‚

-----

## ğŸŒ åŸºäºæ™ºèƒ½ä½“æœç´¢ä¸æ¢ç´¢çš„æ–¹æ³•

è¿™ç±»æ–¹æ³•æ„å»ºäº†èƒ½å¤Ÿä¸»åŠ¨ä¸å¤–éƒ¨ç¯å¢ƒï¼ˆå¦‚ç½‘ç»œã€æ–‡æ¡£ï¼‰äº¤äº’çš„æ™ºèƒ½ä½“ï¼Œé€šè¿‡åŠ¨æ€æœç´¢å’Œä¿¡æ¯æå–æ¥å¼¥è¡¥è‡ªèº«çŸ¥è¯†çš„ä¸è¶³ï¼Œä»è€Œå®Œæˆå¤æ‚çš„ã€çŸ¥è¯†å¯†é›†å‹çš„æ¨ç†ä»»åŠ¡ã€‚

### Agentic Search-Enhanced Large Reasoning Models

  - ğŸ“„ **è®ºæ–‡:** [https://arxiv.org/abs/2501.05366](https://arxiv.org/abs/2501.05366)
  - ğŸ’» **ä»£ç :** [https://github.com/sunnynexus/Search-o1](https://github.com/sunnynexus/Search-o1)
  - **æ‘˜è¦:** **Search-o1** æ¡†æ¶é€šè¿‡ä¸€ç§**æ™ºèƒ½ä½“æ£€ç´¢å¢å¼ºç”Ÿæˆ (agentic RAG)** æœºåˆ¶æ¥å¢å¼ºå¤§å‹æ¨ç†æ¨¡å‹ã€‚å®ƒå°†ä¸€ä¸ªæ™ºèƒ½ä½“æœç´¢å·¥ä½œæµé›†æˆåˆ°æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä½¿æ¨¡å‹åœ¨é‡åˆ°ä¸ç¡®å®šçš„çŸ¥è¯†ç‚¹æ—¶èƒ½å¤ŸåŠ¨æ€æ£€ç´¢å¤–éƒ¨çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è¿˜è®¾è®¡äº†ä¸€ä¸ªç‹¬ç«‹çš„**æ–‡æ¡£å†…æ¨ç† (Reason-in-Documents)** æ¨¡å—ï¼Œç”¨äºåœ¨å°†ä¿¡æ¯æ³¨å…¥æ¨ç†é“¾ä¹‹å‰æ·±å…¥åˆ†ææ£€ç´¢åˆ°çš„å†…å®¹ï¼Œä»¥å‡å°‘å™ªéŸ³å¹¶ä¿æŒæ¨ç†çš„è¿è´¯æ€§ã€‚

### WebThinker: Empowering Large Reasoning Models with Deep Research Capability

  - ğŸ“„ **è®ºæ–‡:** [https://arxiv.org/abs/2504.21776](https://arxiv.org/abs/2504.21776)
  - ğŸ’» **ä»£ç :** [https://github.com/RUC-NLPIR/WebThinker](https://github.com/RUC-NLPIR/WebThinker)
  - **æ‘˜è¦:** **WebThinker** æ˜¯ä¸€ä¸ªæ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“ï¼Œå®ƒä½¿å¤§å‹æ¨ç†æ¨¡å‹èƒ½å¤Ÿè‡ªä¸»æœç´¢ç½‘ç»œã€æµè§ˆç½‘é¡µï¼Œå¹¶åœ¨æ¨ç†è¿‡ç¨‹ä¸­èµ·è‰ç ”ç©¶æŠ¥å‘Šã€‚è¯¥æ¡†æ¶é›†æˆäº†**æ·±åº¦ç½‘é¡µæµè§ˆå™¨ (Deep Web Explorer)** æ¨¡å—ï¼Œå¹¶é‡‡ç”¨**è‡ªä¸»æ€è€ƒ-æœç´¢-èµ·è‰ (Autonomous Think-Search-and-Draft)** ç­–ç•¥ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ— ç¼åœ°äº¤ç»‡æ¨ç†ã€ä¿¡æ¯æ”¶é›†å’ŒæŠ¥å‘Šæ’°å†™ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜é€šè¿‡åŸºäºRLçš„åœ¨çº¿DPOç­–ç•¥æ¥å¢å¼ºç ”ç©¶å·¥å…·çš„åˆ©ç”¨ã€‚

### SciMaster: Towards General-Purpose Scientific AI Agents

  - ğŸ“„ **è®ºæ–‡:** [https://arxiv.org/abs/2507.05241](https://arxiv.org/abs/2507.05241)
  - ğŸ’» **ä»£ç :** [https://github.com/sjtu-sai-agents/X-Master](https://github.com/sjtu-sai-agents/X-Master)
  - **æ‘˜è¦:** **X-Master** æ˜¯ä¸€ä¸ªæ—¨åœ¨æ¨¡æ‹Ÿäººç±»ç ”ç©¶äººå‘˜çš„å·¥å…·å¢å¼ºå‹æ¨ç†æ™ºèƒ½ä½“ï¼Œå®ƒå°†ä»£ç æ¦‚å¿µåŒ–ä¸ºä¸€ç§äº¤äº’è¯­è¨€ï¼Œçµæ´»åœ°åˆ©ç”¨å†…ç½®Pythonåº“å’Œå®šåˆ¶å·¥å…·æ¥å¢å¼ºæ¨ç†ã€‚é€šè¿‡ä¸€ä¸ªåä¸º **X-Masters** çš„åˆ†å¸ƒå¼ã€å †å å¼æ™ºèƒ½ä½“å·¥ä½œæµï¼Œè¯¥ç³»ç»Ÿæ€§åœ°å¢å¼ºäº†æ¨ç†çš„å¹¿åº¦å’Œæ·±åº¦ã€‚å…¶å¼€æºè§£å†³æ–¹æ¡ˆåœ¨ **HLE** åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†32.1%çš„æ–°çºªå½•ï¼Œæˆä¸ºé¦–ä¸ªçªç ´30%é—¨æ§›çš„å·¥ä½œï¼Œè¶…è¶Šäº†OpenAIå’ŒGoogleçš„åŒç±»ç ”ç©¶ã€‚
